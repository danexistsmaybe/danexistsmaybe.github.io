[
  {
    "objectID": "mini2.html",
    "href": "mini2.html",
    "title": "Using API Calls",
    "section": "",
    "text": "The data for this mini project was taken from the openFDA API. This API serves public data from the Food and Drug Administration about drugs, devices, and foods. It only has publicly available data, so there are no sensitive personal details. We hope to use this data to show a correlation between demographic information and certain adverse effects. We also want to see if some drugs are more likely to cause serious side effects compared to other drugs, and what reactions are most common.\nThus, we decided to scrape reports about adverse drug effects on patients, and filter to only include instances where the patient had severe side effects (Death, life threatening condition, hospitalization, etc). Each report has a unique report ID, details about the patients age and sex, what drug they used, how serious the side effects were, and what their reaction to the drug was. We can group the data by age group, sex, medication name, and create plots to show the size of each group and see if there is a causal relationship between 2 or more of the variables.\n\n\nCode\nkey &lt;- readLines(\"api_token\") # get key"
  },
  {
    "objectID": "mini2.html#using-the-data",
    "href": "mini2.html#using-the-data",
    "title": "Using API Calls",
    "section": "Using the Data",
    "text": "Using the Data\nNow that we have our data, we should do something with it! These tree maps will illustrate what the overall distribution among specific drugs looks like, as well as reveal any outliers.\n\n\nCode\nlibrary(treemapify)\n\n\ndrugs |&gt;\n  mutate(drugName = as.character(fct_lump_n(drugName, 100))) |&gt;\n  group_by(drugName) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(drugName = ifelse(\n    nchar(drugName)&gt;7, \n    str_c(substr(drugName, start=1, stop=4), \"…\"),\n    drugName\n  )) |&gt;\n  ggplot(aes(area = n, fill = drugName, label = drugName)) +\n    geom_treemap() +\n    geom_treemap_text(colour = \"white\", place = \"centre\") +\n    labs(title = \"Drug Representation Among Serious Effects\") +\n    theme_minimal() +\n    scale_fill_viridis_d(option = \"G\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\ndrugs |&gt;\n  filter(reaction == \"Death\") |&gt;\n  mutate(drugName = as.character(fct_lump_min(drugName, 1))) |&gt;\n  group_by(drugName) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(drugName = ifelse(\n    nchar(drugName)&gt;10, \n    str_c(substr(drugName, start=1, stop=6), \"…\"),\n    drugName\n  )) |&gt;\n  ggplot(aes(area = n, fill = drugName, label = drugName)) +\n    geom_treemap() +\n    geom_treemap_text(colour = \"white\", place = \"centre\") +\n    labs(title = \"Drug Representation Among Deaths\") +\n    theme_minimal() +\n    scale_fill_viridis_d(option = \"A\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThe plots give some indication of which drugs caused the most serious side effects. In both plots, we can see that the distribution is relatively even among the drugs, as it seems that most of them only occur one or two times. Lipitor stands out as having the most incidents, and we can see that it also has one death associated with it in the second plot. This plot also reveals that Glivec, while it had appeared benign before, actually caused the highest number of deaths in the data.\n\n\nCode\ndrugs |&gt;\n  filter(!is.na(patientAge) & patientAge &lt; 100) |&gt;\n  ggplot(aes(x = patientAge)) +\n    geom_density(fill = \"lavender\", color = \"darkblue\", size = 1) + \n    theme_minimal()\n\n\n\n\n\n\n\n\n\nUnsurprisingly, the most common age for incidents in the data set is in the sixties. However, this plot also highlights that some of the drugs affected children."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome!\nA soon-to-be graduate of St. Olaf College, I am studying Computer Science, Mathematics, English, and Statistics & Data Science. You can find my socials below, or contact me directly at my email address."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "mini1.html#first-map-prevalence-of-depression-across-the-united-states",
    "href": "mini1.html#first-map-prevalence-of-depression-across-the-united-states",
    "title": "Visualizing Maps",
    "section": "First Map: Prevalence of Depression Across the United States",
    "text": "First Map: Prevalence of Depression Across the United States\n\n\nCode\n# First\n\n# summary by state\nPLACES_states &lt;- PLACES |&gt;\n  mutate(State = tolower(State)) |&gt;\n  group_by(State) |&gt;\n  summarize(\n    continuous = mean(DEPRESSION_CrudePrev), \n    other = mean(SLEEP_CrudePrev)\n  )\n\n# join\nchimera &lt;- states |&gt;\n  left_join(PLACES_states, by = c(\"region\" = \"State\"))\n\n# plot\nchimera |&gt;\n  ggplot(aes(x = long, y = lat, group = group)) +\n    geom_polygon(aes(fill = continuous), color = \"#222200\") +\n    coord_map() +  \n    theme_void() +  \n    scale_fill_viridis_c(option = \"E\") +\n    theme(panel.background = element_rect(fill = \"beige\")) +\n    labs(\n      title = \"Average Prevalence of Depression by State\",\n      fill = \"Mean Prevalence of Depression\",\n      caption = \"Source: CDC's PLACES Census Tract Data 2024 Release\\nhttps://data.cdc.gov/500-Cities-Places/PLACES-Census-Tract-Data-GIS-Friendly-Format-2024-/yjkw-uj5s\"\n    )"
  },
  {
    "objectID": "mini1.html#second-map-prevalence-of-lack-of-sleep-across-the-united-states",
    "href": "mini1.html#second-map-prevalence-of-lack-of-sleep-across-the-united-states",
    "title": "Visualizing Maps",
    "section": "Second Map: Prevalence of Lack of Sleep Across the United States",
    "text": "Second Map: Prevalence of Lack of Sleep Across the United States\n\n\nCode\n# First\nAVG &lt;- mean(PLACES_states$other)\n\nchimera &lt;- chimera |&gt;\n  mutate(categorical = ifelse(other&gt;AVG, \"More Than Average\", \"Less Than Average\"))\n\n# plot\nchimera |&gt;\n  ggplot(aes(x = long, y = lat, group = group)) +\n    geom_polygon(aes(fill = fct(categorical)), color = \"#353535\") +\n    coord_map() +  \n    theme_void() +  \n    scale_fill_viridis_d(option = \"F\") +\n    theme(panel.background = element_rect(fill = \"lavender\")) +\n    labs(\n      title = \"U.S. States With Higher Than Average Sleep Deprivation\",\n      subtitle = \"(where the average prevalence of sleep deprivation is 35.82%)\",\n      fill = \"Prevalence of Sleep Deprivation\",\n      caption = \"Source: CDC's PLACES Census Tract Data 2024 Release\\nhttps://data.cdc.gov/500-Cities-Places/PLACES-Census-Tract-Data-GIS-Friendly-Format-2024-/yjkw-uj5s\"\n    )\n\n\n\n\n\n\n\n\n\nDescription: This plot highlights the geographical divide between the states with a higher prevalence of sleep deprivation than average and those with a lower prevalence. The prevalence for each state is calculated by averaging the percentage across its counties; by splitting the states into two categories, it becomes extremely clear that the Southeast as a geographical region has a higher prevalence of lack of sleep. This does not seem to follow the trends observed in the map of depression; however, it would be premature to assume that sleep deprivation and depression are not correlated, as there could be many other factors contributing to the trends observed in these maps."
  },
  {
    "objectID": "mini1.html#first-map-interactive",
    "href": "mini1.html#first-map-interactive",
    "title": "Visualizing Maps",
    "section": "First Map: Interactive",
    "text": "First Map: Interactive\n\n\nCode\n# First\n\n# bins\nbins &lt;- c(15, 17.5, 20, 22.5, 25, 27.5, 30)\npal &lt;- colorBin(\"plasma\", domain = PLACES_states$depr, bins = bins)\n\n# labels\nlibrary(htmltools)\nlibrary(glue)\n\nPLACES_states &lt;- PLACES_states |&gt;\n  mutate(labels = str_c(\"&lt;u&gt;\", name, \"&lt;/u&gt;: \", round(depr,2), \"% prevalence\"))\n\nlabels &lt;- lapply(PLACES_states$labels, HTML)\n\n\n# plot\nleaflet(PLACES_states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles(attribution = \"Source: &lt;a href='https://data.cdc.gov/500-Cities-Places/PLACES-Census-Tract-Data-GIS-Friendly-Format-2024-/yjkw-uj5s'&gt;CDC PLACES&lt;/a&gt;\") |&gt;\n  addPolygons(\n    fillColor = ~pal(depr),\n    weight = 1,\n    opacity = 1,\n    color = \"black\",\n    dashArray = \"\",\n    fillOpacity = .9,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"white\",\n      dashArray = \"\",\n      fillOpacity = .5,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~density, opacity = 0.7, title = \"Prevalence of Depression\",\n    position = \"bottomright\")"
  },
  {
    "objectID": "mini4.html#scraping-the-lyrics",
    "href": "mini4.html#scraping-the-lyrics",
    "title": "Kendrick Lamar Discography Text Analysis",
    "section": "Scraping the Lyrics",
    "text": "Scraping the Lyrics\nThe first task is getting the lyrics from each of his songs. I hand-selected the albums and urls from https://www.musixmatch.com, in order to weed out singles and E.P.s; then, I built a function that scrapes all the individual song urls for one album, iterates through each one, and records the lyrics. Finally, everything is compiled into one dataframe.\n\n\nCode\n# Main function\nscrapealbum &lt;- function(albumtitle, albumurl) {\n  # Create empty tubble\n  L = tibble(\n    album = character(),\n    song = character(),\n    albumyear = integer(),\n    lyrics = character()\n    \n  )\n  \n  # First step: get song urls\n  res &lt;- read_html(albumurl)\n  urls &lt;- res |&gt;\n    html_elements(\"a\") |&gt;\n    html_attr(name = \"href\") |&gt;\n    str_subset(\"/lyrics/\")\n  \n  # Second step: scrape scrape scrape\n  base = \"https://www.musixmatch.com\"\n  for (url in urls) {\n    res &lt;- read_html(str_c(base, url))\n    \n    # title\n    title = res |&gt;\n      html_elements(\"h1\") |&gt;\n      html_text()\n      \n    # year\n    year = res |&gt;\n      html_elements(\"h2\") |&gt;\n      html_text() |&gt;\n      str_subset(\"[0-9][0-9][0-9][0-9]\") |&gt;\n      str_match(\"[0-9][0-9][0-9][0-9]\") |&gt;\n      as.integer()\n    \n    # lyrics\n    lyrics &lt;- res |&gt;\n      html_element(\".css-175oi2r.r-13awgt0.r-eqz5dr.r-1v1z2uz\")\n    lyrics &lt;- lyrics |&gt;\n      html_text2() |&gt;\n      str_remove(\"Writer\\\\(s\\\\)(.|\\n)*$\") |&gt;\n      str_remove(\"^.*\\n\")\n    \n    # add\n    L &lt;- L |&gt;\n      add_row(album = albumtitle, song = title, albumyear = year, lyrics = lyrics)\n  }\n\n  return(L)\n}\n\n# album titles\nalbums = c(\n  \"Overly Dedicated\",\n  \"Section.80\",\n  \"good kid, m.A.A.d city\",\n  \"To Pimp A Butterfly\",\n  \"untitled unmastered.\",\n  \"DAMN.\",\n  \"Mr. Morale & The Big Steppers\",\n  \"GNX\"\n)\n\n# album links\nalbumurls = c(\n  \"https://www.musixmatch.com/album/Kendrick-Lamar/Overly-Dedicated-6\",\n  \"https://www.musixmatch.com/album/Kendrick-Lamar/Section-80-3\",\n  \"https://www.musixmatch.com/album/Kendrick-Lamar/good-kid-m-A-A-d-city-Deluxe-Version-1\",\n  \"https://www.musixmatch.com/album/Kendrick-Lamar/To-Pimp-a-Butterfly-6\",\n  \"https://www.musixmatch.com/album/Kendrick-Lamar/untitled-unmastered-1\",\n  \"https://www.musixmatch.com/album/Kendrick-Lamar/DAMN\",\n  \"https://www.musixmatch.com/album/Kendrick-Lamar/Mr-Morale-The-Big-Steppers\",\n  \"https://www.musixmatch.com/album/Kendrick-Lamar/GNX\"\n)\n\nLyrics &lt;- tibble(\n    album = character(),\n    song = character(),\n    albumyear = integer(),\n    lyrics = character()\n    \n  )\n\n# Main loop\nfor (a in seq_along(albums)) {\n  Lyrics &lt;- bind_rows(Lyrics, scrapealbum(albums[a], albumurls[a]))\n  \n}\n\n# Save it for later\nwrite_csv(Lyrics, \"lyrics.csv\")"
  },
  {
    "objectID": "mini4.html#wrangling-the-lyrics",
    "href": "mini4.html#wrangling-the-lyrics",
    "title": "Kendrick Lamar Discography Text Analysis",
    "section": "Wrangling the Lyrics",
    "text": "Wrangling the Lyrics\nNext, I needed to clean the data up and prepare it for various kinds of analysis. I started by ordering the albums by their associated release year. Then, I created a second tibble where each song was split up into different lines. Finally, I made a third tibble where each song was split up by word, using the str_split and boundary functions. For the second and third tibbles, I forced every character to be lowercase.\n\n\nCode\nlyrics &lt;- read.csv(\"lyrics.csv\")\n\nlyrics &lt;- lyrics |&gt;\n  group_by(album) |&gt;\n  mutate(albumyear = mode_real(albumyear)) |&gt;\n  ungroup() |&gt;\n  mutate(album = fct_reorder(album, albumyear, .fun = mean))\n\nlyrics_lines &lt;- lyrics |&gt;\n  mutate(line = str_split(lyrics, \"\\n\")) |&gt;\n  unnest(line) |&gt;\n  mutate(line = tolower(line)) |&gt;\n  select(-lyrics)\n\nlyrics_words &lt;- lyrics |&gt;\n  mutate(word = str_split(lyrics, boundary(\"word\"))) |&gt;\n  unnest(word) |&gt;\n  mutate(word = tolower(word)) |&gt;\n  select(-lyrics)"
  },
  {
    "objectID": "mini4.html#numerical-analysis",
    "href": "mini4.html#numerical-analysis",
    "title": "Kendrick Lamar Discography Text Analysis",
    "section": "Numerical Analysis",
    "text": "Numerical Analysis\nI began by analyzing several numerical features in the text data.\n\nBars per Song Over Discography\nMany popular rappers today are known for fast-paced, one-note bangers that last two to three minutes and maximize fun and excitement. Kendrick Lamar, however, has several slow-burners in his discography; one might even say that if there was an analogue for the genre of the epic poem in rap, Kendrick might be the one to have created it. Furthermore, trends have changed over time, with popular music getting shorter and shorter; thus, I wanted to see how Kendrick’s discography reflects the amount of language per song.\n\n\nCode\nlyrics_lines |&gt;\n  group_by(album, albumyear, song) |&gt;\n  summarize(n_lines = n()) |&gt;\n  ggplot(aes(x = album, y = n_lines, fill = album, color = album)) +\n    geom_boxplot()  +\n    labs(\n      title = \"Bars per Song\",\n      x = \"Album\",\n      y = \"Number of Lines\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    scale_fill_viridis_d(option = \"magma\") +\n    scale_color_viridis_d(option = \"E\", direction = -1)\n\n\n\n\n\n\n\n\n\nUntitled Unmastered is an outlier here, which may suggest that Kendrick’s songs would be even longer if not for strict refinement and revision. Moreover, it seems that, after the TPaB era, the variation – if not the centers – of the distributions for each album get a lot lower. Every album before DAMN. has at least one very long song, but afterwards the variation is fairly slight, reaching its minimum at GNX.\n\n\nWord Length Over Discography\nLooking at things more granularly, word length may be a reasonable indicator of overall language complexity. Kendrick is known, of course, for having some of the most intricate and complex bars out there, and I wanted to see if this label has truly stuck with him or not. For this plot, I got the best results when taking the distribution of average word length across songs, for each album.\n\n\nCode\nlibrary(ggridges)\nlyrics_words |&gt;\n  mutate(l = str_length(word)) |&gt;\n  group_by(album, song) |&gt;\n  summarize(wlength = mean(l)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = album, x = wlength, fill = album, color = album)) +\n    geom_density_ridges(alpha = .85)  +\n    labs(\n      title = \"Word-Length Per Song, Across Discography\",\n      x = \"Average Word Length\",\n      y = \"Album\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    scale_fill_viridis_d(option = \"D\") +\n    scale_color_viridis_d(option = \"E\", direction = -1)\n\n\n\n\n\n\n\n\n\nThe average word length does seem to reach its highest point on To Pimp A Butterfly, widely considered Kendrick’s master work. However, another interesting trend is that the variation seems to increase dramatically from DAMN. onwards, which may indicate a shift in direction for Kendrick’s writing. In particular, GNX has a very large bimodal distribution, and, since the album is sonically very different from his other work, I wanted to take a closer look.\n\n\nA Closer Look: GNX\nI started by examining GNX on a song-by-song basis, where each black point represents a word, and the larger red points lie on the average for that song. Then, I separated the songs by whether or not they were solos, and compared the distributions.\n\n\nCode\nlongest &lt;- lyrics_words |&gt;\n  filter(album==\"GNX\") |&gt;\n  mutate(wlength = str_length(word)) |&gt;\n  group_by(song) |&gt;\n  summarize(mxword = word[which.max(wlength)], mxlength = str_length(mxword))\n\nlyrics_words |&gt;\n  filter(album==\"GNX\") |&gt;\n  mutate(wlength = str_length(word)) |&gt;\n  group_by(song) |&gt;\n  mutate(mwlength = mean(wlength)) |&gt;\n  ungroup() |&gt;\n  ggplot() +\n    geom_jitter(aes(x = wlength, y = song), alpha = .4) +\n    geom_point(aes(x = mwlength, y = song), size = 4, color = \"red\") +\n    geom_label(data = longest, aes(x = mxlength, y = song, label = mxword)) +\n    labs(title = \"GNX Word Length\", x = \"Word Length\", y = \"Song\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nlyrics_words |&gt;\n  filter(album==\"GNX\") |&gt;\n  mutate(wlength = str_length(word)) |&gt;\n  group_by(song) |&gt;\n  summarize(mwlength = mean(wlength)) |&gt;\n  ungroup() |&gt;\n  mutate(isfeat = ifelse(str_detect(song, \"feat.\"), \"Feature\", \"Solo\")) |&gt;\n  ggplot(aes(x = isfeat, y = mwlength)) +\n    geom_boxplot() +\n    theme_minimal() +\n    labs(\n      title = \"Feature Average Word Length\",\n      y = \"Average Word Length\",\n      x = \"Is Feature?\"\n    )\n\n\n\n\n\n\n\n\n\nIn the first plot, it appears that a lot of the more popular singles off of GNX, like Luther and Squabble Up, have slightly lower word lengths. I also noticed that average word length seemed to be related to features; in the second plot, it is obvious that the songs with features typically have shorter words, so much so that this may be the main contributor to the bimodal distribution visible in the ridgeline plot.\n\n\nExpletives Over Discography\nNext, just for fun, I loaded in a dataset of swear words and vulgar language, to see how Kendrick has used these words over time.\n\n\nCode\nswears &lt;- read.csv(\"vulgar.csv\")\n\nlyrics_words |&gt;\n  group_by(album) |&gt;\n  mutate(wpa = n()) |&gt;\n  ungroup() |&gt;\n  semi_join(swears) |&gt;\n  group_by(album) |&gt;\n  summarize(swearsperwords = n()/mean(wpa)) |&gt;\n  ggplot(aes(x = album, y = swearsperwords, fill = album, color = album)) +\n    geom_col() +\n    labs(\n      title = \"Expletives by Album\",\n      x = \"Album\",\n      y = \"Expletives per Word\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    scale_fill_viridis_d(option = \"F\") +\n    scale_color_viridis_d(option = \"F\", direction = -1)\n\n\n\n\n\n\n\n\n\nCode\nmostvulgar &lt;- lyrics_words |&gt;\n  semi_join(swears) |&gt;\n  group_by(album, albumyear, song) |&gt;\n  summarize(n = n()) |&gt;\n  ungroup() |&gt;\n  arrange(desc(n)) |&gt;\n  slice_head(n = 15)\n\nlibrary(kableExtra)\nkable(mostvulgar, col.names = c(\"Album\", \"Year\", \"Song\", \"Number of Expletives\"))\n\n\n\n\n\n\n\n\n\n\n\nAlbum\nYear\nSong\nNumber of Expletives\n\n\n\n\nOverly Dedicated\n2010\nMichael Jordan\n176\n\n\nMr. Morale & The Big Steppers\n2022\nWe Cry Together\n172\n\n\nSection.80\n2011\nThe Spiteful Chant\n157\n\n\nSection.80\n2011\nTammy’s Song (Her Evils)\n57\n\n\nuntitled unmastered.\n2016\nuntitled 02 | 06.23.2014.\n52\n\n\nuntitled unmastered.\n2016\nuntitled 07 | 2014 - 2016\n52\n\n\nDAMN.\n2017\nELEMENT.\n51\n\n\nOverly Dedicated\n2010\nP&P 1.5\n50\n\n\ngood kid, m.A.A.d city\n2012\nBackseat Freestyle\n49\n\n\ngood kid, m.A.A.d city\n2012\nBitch, Don’t Kill My Vibe - Remix\n49\n\n\ngood kid, m.A.A.d city\n2012\nm.A.A.d city\n46\n\n\nDAMN.\n2017\nHUMBLE.\n46\n\n\ngood kid, m.A.A.d city\n2012\nSing About Me, I’m Dying Of Thirst\n44\n\n\ngood kid, m.A.A.d city\n2012\nThe Art of Peer Pressure\n43\n\n\nTo Pimp A Butterfly\n2015\nFor Free? - Interlude\n41\n\n\n\n\n\nPerhaps unsurprisingly, the number of expletives seems to have decreased as the weight of Kendrick’s subject matter increased. However, it is interesting to see that his vulgar language has jumped back up in the later part of his career. With this and trends of word length in mind, it seems feasible that some sort of change was made starting with DAMN., which is known for having short, simple titles for each song, that nonetheless reference deeply human subjects.\n\n\nTautological Rhymes\nFinally, I used regex to count instances where Kendrick’s lines end with the same word. This can happen for two main reasons: either a line is repeated, as in a chorus, or a tautological rhyme is used, where the same word rhymes with itself.\n\n\nCode\nlyrics |&gt;\n  mutate(ntauts = str_count(lyrics, \"( .+\\n).*\\\\1\")) |&gt;\n  group_by(album) |&gt;\n  summarize(ntauts = mean(ntauts)) |&gt;\n  ggplot(aes(x = album, y = ntauts, fill = album, color = album)) +\n    geom_col() +\n    labs(\n      title = \"Tautological Rhymes Over Discography\",\n      x = \"Album\",\n      y = \"Tautological Rhymes per Song\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    scale_fill_viridis_d(option = \"magma\") +\n    scale_color_viridis_d(option = \"E\", direction = -1)\n\n\n\n\n\n\n\n\n\nSurprisingly, we can see that GKMC has by far the most repeated end words, while GNX has the least. While it would be easy to assume that more tautological lines means a less complex song, it’s important to consider that repetition can be both used and abused; while GKMC may cover less ground per song than GNX, I believe it is more intentionally structured and does a better job driving its messages home."
  },
  {
    "objectID": "mini4.html#sentiment-analysis",
    "href": "mini4.html#sentiment-analysis",
    "title": "Kendrick Lamar Discography Text Analysis",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nSentiment Across Discography\nIn order to look at sentiment in Kendrick’s lyrics, I joined them with Bing’s sentiment lexicon. I calcuated the proportion of negative and positive words for each album, and put the most extreme songs on top of a stacked bar chart.\n\n\nCode\nbing_sentiments &lt;- get_sentiments(lexicon = \"bing\")\n\npwords &lt;- lyrics_words |&gt;\n  inner_join(bing_sentiments) |&gt;\n  group_by(album,song) |&gt;\n  summarize(prop = sum(sentiment == \"positive\")/n()) |&gt;\n  slice_max(order_by = prop)\n\nnwords &lt;- lyrics_words |&gt;\n  inner_join(bing_sentiments) |&gt;\n  group_by(album,song) |&gt;\n  summarize(prop = sum(sentiment == \"negative\")/n()) |&gt;\n  slice_max(order_by = prop)\n\n  \nlyrics_words |&gt;\n  inner_join(bing_sentiments) |&gt;\n  mutate(sentiment = fct_relevel(sentiment, c(\"positive\", \"negative\"))) |&gt;\n  ggplot() +\n    geom_bar(aes(x = album, fill = sentiment), position=\"fill\") +\n    geom_hline(yintercept = .5, color = \"purple\") +\n    geom_label(data = pwords, aes(x = album, y = prop, label = song)) +\n    geom_label(data = nwords, aes(x = album, y = 1-prop, label = song)) +\n    theme_minimal() +\n    scale_fill_viridis_d(option = \"D\") \n\n\n\n\n\n\n\n\n\nAll of Kendrick’s albums are more negative than positive, but still, despite the stereotypes of violence, drugs, and sex that are often associated with rap music, or the association of Kendrick’s music with weighty themes like racism and human suffering, every album gets fairly close to the 50% mark. In fact, Kendrick seems to have gotten his most optimistic on DAMN., which is an extremely polarized album.\n\n\nWord Clouds\nI wanted to look at the words of K.’s most positive and most negative songs a little more closely. The first word cloud I made is for LOVE., which may be the most positive song simply because of how often love is mentioned; the second wordcloud is for We Cry Together, which, if you have listened to this song, you know why it would be the most negative. Anyone that has listened to it would also be aware that a lot of the song is made up of expletives, so I decided to remove most of the words to ensure my wordcloud didn’t look like a drunk man’s post-toe stubbing speech bubble.\n\nLOVE.\n\n\nCode\nlibrary(htmlwidgets)\nwc1 &lt;- lyrics_words |&gt;\n  filter(song == \"LOVE. FEAT. ZACARI.\") |&gt;\n  anti_join(stop_words) |&gt;\n  mutate(tot = n()) |&gt;\n  group_by(word) |&gt;\n  summarize(freq = n()/mean(tot)) |&gt;\n  wordcloud2(size = 1, shape = \"hexagon\", minSize = 1, widgetsize = c(800,600))\n\nsaveWidget(wc1, \"wc1.html\")\n\n\n\n\n\n\nWe Cry Together\n\n\nCode\nw2 &lt;- lyrics_words |&gt;\n  filter(song == \"We Cry Together\") |&gt;\n  anti_join(stop_words) |&gt;\n  anti_join(swears) |&gt;\n  mutate(tot = n()) |&gt;\n  group_by(word) |&gt;\n  summarize(freq = n()/mean(tot)) |&gt;\n  wordcloud2(widgetsize = c(800, 800))\nsaveWidget(w2, \"wc2.html\")"
  },
  {
    "objectID": "mini4.html#conclusion",
    "href": "mini4.html#conclusion",
    "title": "Kendrick Lamar Discography Text Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn general, the variation over Kendrick’s discography does not change in dramatic and remarkable ways; through it all, Kendrick has remained the same ambitious artist we all know. Nonetheless, there are features worth further examination. Mainly, many trends indicate a shift starting with DAMN., which aligns with a major moment in Kendrick’s career, after receiving the highest accolades for TPaB and becoming the darling of the intellectual music industry. DAMN. is, in many ways, Kendrick’s transition into becoming a part of popular music, and his reckoning with the darker significance of that transition, which remains unresolved to this day. Mr. Morale & The Big Steppers, his most negative album, sees him grappling with this in particular. Still, GNX sees him moving closer to a middle line, and, perhaps, we will see a new era for Kendrick in the near future."
  },
  {
    "objectID": "mini4.html",
    "href": "mini4.html",
    "title": "Kendrick Lamar Discography Text Analysis",
    "section": "",
    "text": "It is widely considered that, last year, Kendrick Lamar toppled Drake and established himself as the biggest rapper in modern hip-hop. While being the biggest makes him marginally less interesting, his career has reached an auspicious moment: it has acquired ‘legacy.’ K. has been in the game for more than two decades now, and, with eight critically-acclaimed albums, the legacy of Kendrick Lamar is just now something worth discussing. That is why, for my text analysis project, I chose to study the varied lyrics of Kendrick’s discography, from when he was a rambunctious gang-banging youth all the way to his current image as the Shepherd of black culture, poet of America, and father of two."
  },
  {
    "objectID": "mini4.html#introduction",
    "href": "mini4.html#introduction",
    "title": "Kendrick Lamar Discography Text Analysis",
    "section": "",
    "text": "It is widely considered that, last year, Kendrick Lamar toppled Drake and established himself as the biggest rapper in modern hip-hop. While being the biggest makes him marginally less interesting, his career has reached an auspicious moment: it has acquired ‘legacy.’ K. has been in the game for more than two decades now, and, with eight critically-acclaimed albums, the legacy of Kendrick Lamar is just now something worth discussing. That is why, for my text analysis project, I chose to study the varied lyrics of Kendrick’s discography, from when he was a rambunctious gang-banging youth all the way to his current image as the Shepherd of black culture, poet of America, and father of two."
  }
]